Metadata-Version: 2.4
Name: llava
Version: 1.7.0.dev0
Summary: LLaVA OneVision: The Next Generation of LLaVA with Better Image and Video Understanding Capabilities
Home-page: https://llava-vl.github.io
Project-URL: Homepage, https://llava-vl.github.io
Project-URL: Bug Tracker, https://github.com/haotian-liu/LLaVA/issues
Classifier: Programming Language :: Python :: 3
Classifier: License :: OSI Approved :: Apache Software License
Requires-Python: >=3.8
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: torch==2.1.2
Requires-Dist: numpy<2.0
Provides-Extra: standalone
Requires-Dist: shortuuid; extra == "standalone"
Requires-Dist: httpx==0.24.0; extra == "standalone"
Requires-Dist: einops; extra == "standalone"
Requires-Dist: ftfy; extra == "standalone"
Provides-Extra: train
Requires-Dist: llava[standalone]; extra == "train"
Requires-Dist: pynvml==11.5.0; extra == "train"
Requires-Dist: numpy<2.0; extra == "train"
Requires-Dist: open_clip_torch; extra == "train"
Requires-Dist: fastapi; extra == "train"
Requires-Dist: markdown2[all]; extra == "train"
Requires-Dist: requests; extra == "train"
Requires-Dist: sentencepiece; extra == "train"
Requires-Dist: torch==2.1.2; extra == "train"
Requires-Dist: torchvision; extra == "train"
Requires-Dist: uvicorn; extra == "train"
Requires-Dist: wandb==0.18.7; extra == "train"
Requires-Dist: deepspeed==0.14.4; extra == "train"
Requires-Dist: peft==0.4.0; extra == "train"
Requires-Dist: accelerate==0.29.3; extra == "train"
Requires-Dist: tokenizers<0.22; extra == "train"
Requires-Dist: transformers==4.51.0; extra == "train"
Requires-Dist: bitsandbytes; extra == "train"
Requires-Dist: scikit-learn==1.2.2; extra == "train"
Requires-Dist: sentencepiece~=0.1.99; extra == "train"
Requires-Dist: einops==0.6.1; extra == "train"
Requires-Dist: einops-exts==0.0.4; extra == "train"
Requires-Dist: gradio_client==0.2.9; extra == "train"
Requires-Dist: urllib3<=2.0.0; extra == "train"
Requires-Dist: datasets==2.16.1; extra == "train"
Requires-Dist: pydantic==1.10.8; extra == "train"
Requires-Dist: timm; extra == "train"
Requires-Dist: hf_transfer; extra == "train"
Requires-Dist: opencv-python; extra == "train"
Requires-Dist: av; extra == "train"
Requires-Dist: decord; extra == "train"
Requires-Dist: tyro; extra == "train"
Requires-Dist: scipy; extra == "train"
Requires-Dist: kornia; extra == "train"
Requires-Dist: omegaconf; extra == "train"
Requires-Dist: pytorch-lightning; extra == "train"
Requires-Dist: tqdm; extra == "train"
Requires-Dist: torchmetrics; extra == "train"
Requires-Dist: matplotlib; extra == "train"
Requires-Dist: hydra-core; extra == "train"
Requires-Dist: memory_profiler; extra == "train"
Dynamic: classifier
Dynamic: description
Dynamic: description-content-type
Dynamic: home-page
Dynamic: license-file
Dynamic: project-url
Dynamic: provides-extra
Dynamic: requires-dist
Dynamic: requires-python
Dynamic: summary

<div align="center">
  
# LLaVA-UHD-v3

**PROGRESSIVE VISUAL COMPRES-SION FOR EFFICIENT NAIVE-RESOLUTION ENCODING IN MLLMS**
</div>

This repository hosts the code, data of **LLaVA-UHD-v3**, a multimodal large language model (MLLM) built upon our proposed Progressive Visual Compression (PVC) for efficient naive-resolution encoding. Our model not only achieves performance comparable to advanced MLLMs like Qwen2-VL across 15 diverse benchmarks but also delivers a 1.9x reduction in Time-to-First-Token (TTFT). Moreover, LLaVA-UHD v3 can be trained efficiently in academic settings, requiring approximately 300 hours on 32 A100 GPUs. For more details, please visit our ðŸ“ƒ [paper](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/11080.pdf) here


## Overview

![The LLaVA-UHD-v3 framework](LLaVA_UHD_v3.png)

LLaVA-UHD v3 introduces Progressive Visual Compression (PVC) with two key components to enable efficient naive-resolution encoding in MLLMs:

-  An adaptive patch embedding (APE) module that flexibly scales patch sizes, allowing for more fine-grained and detailed visual tokenization while maintaining compatibility with pretrained Vision Transformers.
-  A windowed token compression (WTC) module that progressively merges local token representations within the vision encoder, hierarchically reducing sequence length and computational overhead.

This approach avoids slice-based encoding, thereby preserving holistic visual context and preventing the fragmented semantic understanding often found in other methods. Extensive evaluations show that LLaVA-UHD v3 achieves competitive performance against state-of-the-art models like Qwen2-VL while significantly reducing inference latency.
Demonstrates a superior trade-off between computational efficiency and performance across numerous vision-language benchmarks.

## Release
-[2025/11/05] ðŸ§ The Pilot experiment related code and benchmark mentioned in the paper are available in [hugging-face](https://huggingface.co/ZzzHelloWorld/Pilot_experiment).The checkpoints for global naive-resolution visual encoding ([GNE](https://huggingface.co/ZzzHelloWorld/llava-uhd-final)) and slice-based encoding ([SBE](https://huggingface.co/ZzzHelloWorld/llava_uhd_resampler_query_49)) have also been released.

-[2025/11/05] ðŸ”¥LLaVA-UHD v3 achieves a superior trade-off between efficiency and performance across 15 diverse benchmarks. Our novel vision encoder, ViT-UHD with Progressive Visual Compression (PVC), enables efficient naive-resolution encoding, significantly reducing latency while maintaining competitive performance. Model checkpoints are available in [hugging-face](https://huggingface.co/ZzzHelloWorld/llava-uhd-final).

-[2025/11/01] ðŸ“¢[LLaVA-UHD-v3](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/11080.pdf) is accepted by ICLR2026.

## Environment Preparing
1. To reproduce the results of the paper, please set up the Python environment using the following code:
```bash
conda create -n LLaVA-UHD-v3 python=3.10
conda activate LLaVA-UHD-v3
bash setup_train_env.sh
```

2. Download the checkpoints of [MoonViT-p10](https://huggingface.co/ZzzHelloWorld/moonvit-so-400m-4-18-se-hirope2d-p10) 
   and [SigLIP2-p8](https://huggingface.co/ZzzHelloWorld/siglip2-so400m-patch8-naflex-swin-4-18-contiguous) distilled via the APE method. You also need to download the checkpoints of [Qwen2-7B](https://huggingface.co/Qwen/Qwen2-7B-Instruct) as the LLM.

If something wrong happens, please kindly refer to the issues of [LLaVA](https://github.com/haotian-liu/LLaVA/issues) 
or submit issues in our repository.

## Training Script
Please refer to train.sh for pretraining script and fine-tuning script (we comment in the file). 
If you want to do end-to-end pretraining and fine-tuning, please run the following command.

```bash
sh train.sh
```
In addition, we also provide train_moonvit.sh, which contains training script references for pretraining, OCR + image-text interleaving, and SFT.

## Evaluation Code
Evaluation script is in VLMEvalkit, you need to add the corresponding files to the official VLMEvalkit project for testing.

For details of data organization, please refer to [here](https://github.com/open-compass/VLMEvalKit) for help. 
We provide the same script to complete the testing.

## Citation

If you find LLaVA-UHD-v3 useful for your research and applications, please cite using this BibTeX:
```bibtex
@inproceedings{guo2024llava-uhd,
  title={{LLaVA-UHD}: an LMM Perceiving Any Aspect Ratio and High-Resolution Images},
  author={Guo, Zonghao and Xu, Ruyi and Yao, Yuan and Cui, Junbo and Ni, Zanlin and Ge, Chunjiang and Chua, Tat-Seng and Liu, Zhiyuan and Huang, Gao},
  booktitle={ECCV},
  year={2024}
}
```

